# Pinot Configuration Files
## Overview
This directory contains Pinot schema and table configurations for restaurant order analytics.\n\n
## Tables
### 1. orders_raw_upsert\n**Purpose**: Raw event storage with deduplication\n- **Primary Keys**: event_id is used for deduplication via dedupConfig\n- **Upsert**: Full upsert mode based on timestamp\n- **Retention**: 7 days\n- **Features**:\n  - Bloom filters on event_id and order_id for fast lookups\n  - Inverted indexes on frequently queried dimensions\n  - Sorted by restaurant_id and timestamp for efficient queries\n  - Computed column: revenue_cents = quantity * price_in_cents\n\n### 2. orders_1m_rollup\n**Purpose**: 1-minute aggregated data with star-tree indexes\n- **Primary Keys**: restaurant_id, menu_item_id, window_start_1m\n- **Upsert**: Partial upsert with incremental aggregation\n- **Retention**: 30 days\n- **Features**:\n  - Star-tree index for fast OLAP queries\n  - HLL sketch for unique customer counting\n  - Hour and day dimensions for temporal analysis\n  - Merge rollup task for compaction\n\n## Deployment\n\n### Step 1: Create Schemas\n```bash\n# Create raw upsert schema\ncurl -X POST \"http://localhost:9000/schemas\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @orders_raw_upsert_schema.json\n\n# Create 1m rollup schema\ncurl -X POST \"http://localhost:9000/schemas\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @orders_1m_rollup_schema.json\n```\n\n### Step 2: Create Tables\n```bash\n# Create raw upsert table\ncurl -X POST \"http://localhost:9000/tables\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @orders_raw_upsert_tableConfig.json\n\n# Create 1m rollup table\ncurl -X POST \"http://localhost:9000/tables\" \\\n  -H \"Content-Type: application/json\" \\\n  -d @orders_1m_rollup_tableConfig.json\n```\n\n## Sample Queries\n\n### Top 10 Menu Items by Revenue (Raw Table)\n```sql\nSELECT \n  menu_item_id,\n  menu_item_name,\n  SUM(revenue_cents) as total_revenue_cents,\n  SUM(quantity) as total_quantity,\n  COUNT(*) as order_count\nFROM orders_raw_upsert\nWHERE timestamp > ago('PT24H')\nGROUP BY menu_item_id, menu_item_name\nORDER BY total_revenue_cents DESC\nLIMIT 10\n```\n\n### Top 10 Menu Items by Revenue (Rollup Table)\n```sql\nSELECT \n  menu_item_id,\n  menu_item_name,\n  SUM(sum_revenue_cents) as total_revenue_cents,\n  SUM(sum_quantity) as total_quantity,\n  SUM(order_count) as total_orders,\n  DISTINCTCOUNTHLL(unique_customers_hll) as unique_customers\nFROM orders_1m_rollup\nWHERE window_start_ts > ago('PT24H')\nGROUP BY menu_item_id, menu_item_name\nORDER BY total_revenue_cents DESC\nLIMIT 10\n```\n\n### Revenue by Hour of Day\n```sql\nSELECT \n  hour_of_day,\n  SUM(sum_revenue_cents) / 100.0 as revenue_dollars,\n  SUM(order_count) as orders\nFROM orders_1m_rollup\nWHERE window_start_ts > ago('PT7D')\nGROUP BY hour_of_day\nORDER BY hour_of_day\n```\n\n## Configuration Notes\n\n### Upsert Configuration\n- **orders_raw_upsert**: Uses `dedupConfig` for exact duplicate removal based on event_id\n- **orders_1m_rollup**: Uses partial upsert with INCREMENT strategy for metrics\n\n### Performance Tuning\n- Star-tree indexes enable sub-second OLAP queries on rollup table\n- Bloom filters reduce I/O for point lookups\n- Sorted columns improve compression and query performance\n- Range indexes on timestamp columns for efficient time-based filtering\n\n### Data Pipeline\n1. Raw events → Kafka topic: `restaurant-orders`\n2. Flink job aggregates → Kafka topic: `restaurant-orders-1m-rollup`\n3. Both topics consumed by respective Pinot tables\n"